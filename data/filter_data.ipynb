{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import os\n",
    "import PIL\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import argparse\n",
    "import yaml\n",
    "import torch.nn.functional as F\n",
    "from pathlib import Path\n",
    "from transformers import CLIPProcessor, CLIPModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"bos_token_id\"]` will be overriden.\n",
      "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"eos_token_id\"]` will be overriden.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"openai/clip-vit-large-patch14\"\n",
    "processor = CLIPProcessor.from_pretrained(model_name)\n",
    "model = CLIPModel.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = \"./food/food/japan/\"\n",
    "train_dir = \"/usr1/manhbaon/hw/vlr/img2img-turbo/data/food/train_B\"\n",
    "test_dir = \"/usr1/manhbaon/hw/vlr/img2img-turbo/data/food/test_B\"\n",
    "prompt_filter = \"This is a photo of an authentic Japanese food.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files(directory):\n",
    "    return [str(file.resolve()) for file in Path(directory).iterdir() if file.is_file()]\n",
    "\n",
    "def load_image(path):\n",
    "    image = PIL.Image.open(path)\n",
    "    image = PIL.ImageOps.exif_transpose(image)\n",
    "    image = image.convert(\"RGB\")\n",
    "    return image\n",
    "\n",
    "def load_batch_image(list_path):\n",
    "    images = []\n",
    "    paths = []\n",
    "    for path in list_path:\n",
    "        try:\n",
    "            images.append(load_image(path))\n",
    "            paths.append(path)\n",
    "        except:\n",
    "            continue\n",
    "    assert len(images) == len(paths)\n",
    "    return images, paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_image_paths = list_files(input_dir)\n",
    "\n",
    "selected_image_paths = [] \n",
    "selected_sims = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 16 processed !\n",
      " 4 processed !\n"
     ]
    }
   ],
   "source": [
    "batch_size = 16\n",
    "\n",
    "for i in range(0, len(all_image_paths), batch_size):\n",
    "    images_batch, images_path = load_batch_image(all_image_paths[i:min(i + batch_size, len(all_image_paths))])\n",
    "    selected_image_paths.extend(images_path) \n",
    "    assert len(images_batch) == len(images_path)\n",
    "\n",
    "    prompt_batch = [prompt_filter] * len(images_batch) \n",
    "    prompt_features = processor(text = prompt_batch, images = None, return_tensors=\"pt\", padding = True)\n",
    "    prompt_features = model.get_text_features(**prompt_features).cuda() \n",
    "\n",
    "    image_inputs = processor(text = None, images = images_batch, return_tensors = \"pt\", padding = True)\n",
    "    image_features = model.get_image_features(**image_inputs).cuda()\n",
    "\n",
    "    selected_sims.extend(F.cosine_similarity(prompt_features, image_features).cpu().tolist())\n",
    "\n",
    "    print(f\" {len(images_batch)} processed !\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "topk = 600 \n",
    "topk_indices = sorted(range(len(selected_image_paths)), key=lambda i: selected_sims[i], reverse=True)[:topk]\n",
    "topk_paths = [selected_image_paths[i] for i in topk_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_train_test_paths(paths, split_ratio = 0.8):\n",
    "    # Shuffle the data randomly\n",
    "    random.shuffle(paths)\n",
    "    # Calculate the split index\n",
    "    split_index = int(len(paths) * split_ratio)\n",
    "    # Split the data into train and test sets\n",
    "    train_set = paths[:split_index]\n",
    "    test_set = paths[split_index:]\n",
    "    return train_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil \n",
    "\n",
    "train_paths, test_paths = get_train_test_paths(topk_paths)\n",
    "\n",
    "for train_path in train_paths:\n",
    "    shutil.copy(train_path, train_dir)\n",
    "\n",
    "for test_path in test_paths:\n",
    "    shutil.copy(test_path, test_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transcreation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
